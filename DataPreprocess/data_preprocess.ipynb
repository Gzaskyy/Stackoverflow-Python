{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bs4 \n",
    "# http://omz-software.com/pythonista/docs/ios/beautifulsoup_guide.html\n",
    "\n",
    "! pip install contractions\n",
    "# https://pypi.org/project/contractions/\n",
    "\n",
    "! pip install autocorrect \n",
    "# https://pypi.org/project/autocorrect/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic librairies\n",
    "import time as time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gc \n",
    "# Garbage Collector interface. https://docs.python.org/3/library/gc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text librairies\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag.util import untag\n",
    "\n",
    "import contractions \n",
    "# Fixes contractions such as `you're` to you `are`. https://pypi.org/project/contractions/\n",
    "\n",
    "from autocorrect import Speller "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types. https://numpy.org/devdocs/user/basics.types.html\n",
    "\n",
    "dtypes_questions = {'Id':'int32', 'Score': 'int16', 'Title': 'str', 'Body': 'str'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv('../datasets/Questions.csv', \n",
    "                            usecols=['Id', 'Score', 'Title', 'Body'],\n",
    "                            encoding=\"ISO-8859-1\",\n",
    "                            dtype=dtypes_questions,\n",
    "                            nrows=100                           \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions[['Title', 'Body']] = df_questions[['Title', 'Body']].applymap(lambda x: str(x).encode(\"utf-8\", errors='surrogatepass').decode(\"ISO-8859-1\", errors='surrogatepass'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all questions that have a negative score\n",
    "df_questions = df_questions[df_questions[\"Score\"] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Speller()\n",
    "token = ToktokTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "charac = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789'\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "adjective_tag_list = set(['JJ','JJR', 'JJS', 'RBR', 'RBS']) # List of Adjective's tag from nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Noise removal\n",
    "(removing anythings that can interfere with your text analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Removing html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parse question and title then return only the text\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\'\", \"'\", text) # match all literal apostrophe pattern then replace them by a single whitespace\n",
    "    text = re.sub(r\"\\n\", \" \", text) # match all literal Line Feed (New line) pattern then replace them by a single whitespace\n",
    "    text = re.sub(r\"\\xa0\", \" \", text) # match all literal non-breakable space pattern then replace them by a single whitespace\n",
    "    text = re.sub('\\s+', ' ', text) # match all one or more whitespace then replace them by a single whitespace\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: clean_text(x)) \n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions['Body'][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Remove contractions\n",
    "(expand shortened words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"expand shortened words, e.g. 'don't' to 'do not'\"\"\"\n",
    "    text = contractions.fix(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: expand_contractions(x)) \n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions['Body'][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Spelling correction(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect(text):\n",
    "    words = token.tokenize(text)\n",
    "    words_correct = [spell(w) for w in words]\n",
    "    return ' '.join(map(str, words_correct)) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df_questions['Title'] = df_questions['Title'].apply(lambda x: autocorrect(x)) \n",
    "# df_questions['Body'] = df_questions['Body'].apply(lambda x: autocorrect(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Lowering the text\n",
    "(Lowering the text is a classical and useful step of Noise removal or Text normalization since it reduce the vocabulary, normalize the text and cost almost nothing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.11 ms, sys: 134 Âµs, total: 1.25 ms\n",
      "Wall time: 1.18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_questions['Title'] = df_questions['Title'].str.lower()\n",
    "df_questions['Body'] = df_questions['Body'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have got a menu in python. that part was easy. i am using raw_input() to get the selection from the user. the problem is that raw_input (and input) require the user to press enter after they make a selection. is there any way to make the program act immediately upon a keystroke? here is what i have got so far: import sys print \"\"\"menu 1) say foo 2) say bar\"\"\" answer = raw_input(\"make a selection> \") if \"1\" in answer: print \"foo\" elif \"2\" in answer: print \"bar\" it would be great to have something like print menu while lastkey = \"\": lastkey = check_for_recent_keystrokes() if \"1\" in lastkey: #do stuff...'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Removing character\n",
    "How does Punctuation Affect Neural Models in Natural Language\n",
    "Inference. https://aclanthology.org/2020.pam-1.15.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Removing all non-alphabetical character(Optional)\n",
    "(This step will remove ALL non-alphabetical character, inculding punctuation, munber, special character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_and_number(text):\n",
    "    \"\"\"remove all punctuation and number\"\"\"\n",
    "    return text.translate(str.maketrans(\" \", \" \", charac)) \n",
    "\n",
    "\n",
    "\n",
    "def remove_non_alphabetical_character(text):\n",
    "    \"\"\"remove all non-alphabetical character\"\"\"\n",
    "    text = re.sub(\"[^a-z]+\", \" \", text) # remove all non-alphabetical character\n",
    "    text = re.sub(\"\\s+\", \" \", text) # remove whitespaces left after the last operation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df_questions['Title'] = df_questions['Title'].apply(lambda x: remove_non_alphabetical_character(x)) \n",
    "# df_questions['Body'] = df_questions['Body'].apply(lambda x: remove_non_alphabetical_character(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Removing single character (optional)\n",
    "Also have no idea if I should remove all single characters, since single character always take no meaning. such as a = \"123\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_letter(text):\n",
    "    \"\"\"remove single alphabetical character\"\"\"\n",
    "    text = re.sub(r\"\\b\\w{1}\\b\", \"\", text) # remove all single letter\n",
    "    text = re.sub(\"\\s+\", \" \", text) # remove whitespaces left after the last operation\n",
    "    text = text.strip(\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df_questions['Title'] = df_questions['Title'].apply(lambda x: remove_single_letter(x)) \n",
    "# df_questions['Body'] = df_questions['Body'].apply(lambda x: remove_single_letter(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Removing most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"remove common words in english by using nltk.corpus's list\"\"\"\n",
    "    words = token.tokenize(text)\n",
    "    filtered = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    return ' '.join(map(str, filtered)) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df_questions['Title'] = df_questions['Title'].apply(lambda x: remove_stopwords(x))\n",
    "# df_questions['Body'] = df_questions['Body'].apply(lambda x: remove_stopwords(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Removing adjectives (optional)\n",
    "Maybe I should remove all adjectives as the adjectives won't add any useful infomation since all questions are related to Python programming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_by_tag(text, undesired_tag):\n",
    "    \"\"\"remove all words by using ntk tag (adjectives, verbs, etc.)\"\"\"\n",
    "    words = token.tokenize(text) # Tokenize each words\n",
    "    words_tagged = nltk.pos_tag(tokens=words, tagset=None, lang='eng') # Tag each words and return a list of tuples (e.g. (\"have\", \"VB\"))\n",
    "    filtered = [w[0] for w in words_tagged if w[1] not in undesired_tag] # Select all words that don't have the undesired tags\n",
    "    \n",
    "    return ' '.join(map(str, filtered)) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/zhi/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 496 ms, sys: 2.3 ms, total: 498 ms\n",
      "Wall time: 497 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: remove_by_tag(x, adjective_tag_list))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: remove_by_tag(x, adjective_tag_list))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37561e31f4ee9162b5a364b98c30ccab016159a3db8546ebaae00bdc218e18be"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
